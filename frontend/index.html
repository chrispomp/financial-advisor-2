<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Citi AI Wealth Advisor</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <style>
        :root { --primary-blue: #003B70; --primary-red: #D9261C; --neutral-bg: #FFFFFF; --neutral-light-gray: #F0F2F5; --status-green: #28a745; --font-family: 'Helvetica Neue', Arial, sans-serif; }
        body { background-color: var(--neutral-light-gray); font-family: var(--font-family); margin: 0; padding: 20px; display: flex; justify-content: center; align-items: center; min-height: 100vh; }
        .mobile-container { width: 100%; max-width: 400px; height: 85vh; max-height: 750px; background-color: var(--neutral-bg); border-radius: 20px; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1); overflow: hidden; display: flex; flex-direction: column; }
        header { padding: 15px 20px; background-color: var(--primary-blue); text-align: center; flex-shrink: 0; }
        header img { height: 30px; }
        main { flex-grow: 1; position: relative; background-color: #000; display: flex; flex-direction: column; }
        .video-container { position: relative; width: 100%; flex-grow: 1; }
        #video-feed { width: 100%; height: 100%; object-fit: cover; }
        .placeholder { width: 100%; height: 100%; display: flex; justify-content: center; align-items: center; flex-direction: column; color: #ccc; background-color: #2c2c2c; position: absolute; top: 0; left: 0; }
        .placeholder i { font-size: 80px; margin-bottom: 20px; }
        #agent-transcript { position: absolute; bottom: 20px; left: 20px; right: 20px; background-color: rgba(0, 0, 0, 0.6); color: white; padding: 15px; border-radius: 10px; font-size: 16px; min-height: 50px; opacity: 0; transition: opacity 0.3s; }
        .status-overlay { position: absolute; top: 10px; left: 10px; background-color: rgba(0, 0, 0, 0.5); color: white; padding: 5px 10px; border-radius: 15px; font-size: 14px; display: flex; align-items: center; }
        .status-dot { width: 10px; height: 10px; border-radius: 50%; background-color: var(--primary-red); margin-right: 8px; }
        .status-dot.connected { background-color: var(--status-green); }
        .controls { padding: 20px; background-color: #000; display: flex; justify-content: center; gap: 20px; flex-shrink: 0; }
        .control-button { width: 60px; height: 60px; border-radius: 50%; border: none; cursor: pointer; font-size: 24px; color: white; display: flex; justify-content: center; align-items: center; transition: background-color 0.2s; }
        #startStopBtn { background-color: var(--primary-blue); }
        #startStopBtn.active { background-color: var(--primary-red); }
        #muteBtn { background-color: rgba(255, 255, 255, 0.3); }
        #muteBtn:disabled { background-color: rgba(255, 255, 255, 0.1); cursor: not-allowed; }
    </style>
</head>
<body>
    <div class="mobile-container">
        <header><img src="https://www.citi.com/mobile-apps/assets/img/citi-logo-white.svg" alt="Citi Logo"></header>
        <main>
            <div class="video-container">
                <div class="placeholder" id="video-placeholder"><i class="fa-solid fa-video-slash"></i><p>Session not started</p></div>
                <video id="video-feed" autoplay muted playsinline style="display: none;"></video>
                <div id="agent-transcript"></div>
                <div class="status-overlay">
                    <div id="status-dot" class="status-dot"></div><span id="status-text">Disconnected</span>
                </div>
            </div>
            <div class="controls">
                <button id="muteBtn" class="control-button" disabled><i class="fa-solid fa-microphone"></i></button>
                <button id="startStopBtn" class="control-button"><i class="fa-solid fa-phone"></i></button>
            </div>
        </main>
    </div>

    <script>
        const ui = {
            startStopBtn: document.getElementById('startStopBtn'),
            muteBtn: document.getElementById('muteBtn'),
            videoFeed: document.getElementById('video-feed'),
            statusText: document.getElementById('status-text'),
            statusDot: document.getElementById('status-dot'),
            placeholder: document.getElementById('video-placeholder'),
            transcript: document.getElementById('agent-transcript'),
        };

        const initialState = {
            isConversationActive: false,
            isMuted: false,
            localStream: null,
            socket: null,
            audioContext: null,
            audioProcessor: null,
            audioBufferQueue: [], // Buffer for incoming audio
        };

        let state = { ...initialState };

        const serverUrl = 'http://localhost:8000';

        ui.startStopBtn.addEventListener('click', () => state.isConversationActive ? endConversation() : startConversation());
        ui.muteBtn.addEventListener('click', toggleMute);

        // --- Core Conversation Logic ---

        async function startConversation() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: { sampleRate: 16000, channelCount: 1 } });
                
                state.localStream = stream;
                ui.videoFeed.srcObject = stream;
                ui.videoFeed.style.display = 'block';
                ui.placeholder.style.display = 'none';

                await connectToAgent(stream);

                state.isConversationActive = true;
                ui.startStopBtn.classList.add('active');
                ui.startStopBtn.querySelector('i').className = 'fa-solid fa-phone-slash';
                ui.muteBtn.disabled = false;
            } catch (error) {
                console.error("âŒ Failed to start conversation:", error);
                alert("Could not start. Check console for errors.");
                endConversation();
            }
        }

        function endConversation() {
            state.localStream?.getTracks().forEach(track => track.stop());
            state.socket?.close();
            state.audioContext?.close();
            
            ui.videoFeed.srcObject = null;
            ui.videoFeed.style.display = 'none';
            ui.placeholder.style.display = 'flex';
            ui.transcript.textContent = '';
            ui.transcript.style.opacity = 0;
            ui.statusText.textContent = 'Disconnected';
            ui.statusDot.classList.remove('connected');

            // Reset state object
            state = { ...initialState };

            ui.startStopBtn.classList.remove('active');
            ui.startStopBtn.querySelector('i').className = 'fa-solid fa-phone';
            ui.muteBtn.disabled = true;
            ui.muteBtn.querySelector('i').className = 'fa-solid fa-microphone';
        }

        // --- Network and Media Setup ---

        function connectToAgent(stream) {
            return new Promise(async (resolve, reject) => {
                const wsUrl = serverUrl.replace('http', 'ws');
                state.socket = new WebSocket(`${wsUrl}/run_live`);
                state.socket.binaryType = "arraybuffer";

                state.socket.onopen = async () => {
                    console.log("âœ… WebSocket connected. Initializing audio processor.");
                    ui.statusText.textContent = 'Connected';
                    ui.statusDot.classList.add('connected');
                    try {
                        // Start processing microphone audio and sending it to the agent
                        await setupMicrophoneProcessing(stream);
                        resolve();
                    } catch (e) { reject(e); }
                };

                state.socket.onmessage = handleAgentMessage;
                state.socket.onerror = (err) => { console.error("âŒ WebSocket Error:", err); reject(err); };
                state.socket.onclose = () => { console.log("ðŸ”Œ WebSocket closed."); if(state.isConversationActive) endConversation(); };
            });
        }

        async function setupMicrophoneProcessing(stream) {
            state.audioContext = new AudioContext({ sampleRate: 16000 });
            await state.audioContext.audioWorklet.addModule('audio-processor.js');
            state.audioProcessor = new AudioWorkletNode(state.audioContext, 'audio-processor');
            
            const source = state.audioContext.createMediaStreamSource(stream);
            source.connect(state.audioProcessor);

            // Listen for processed audio chunks from the worklet
            state.audioProcessor.port.onmessage = event => {
                if (state.socket && state.socket.readyState === WebSocket.OPEN) {
                    // Send raw audio data directly
                    state.socket.send(event.data);
                }
            };
        }

        // --- Agent Message Handling ---

        function handleAgentMessage(event) {
            // If the message is binary data, it's audio. Buffer it.
            if (event.data instanceof ArrayBuffer) {
                state.audioBufferQueue.push(event.data);
                return;
            }
            
            // Otherwise, it's a text message (JSON string)
            const message = JSON.parse(event.data);
            console.log("Agent Message:", message);

            if (message.text) {
                ui.transcript.textContent = message.text;
                ui.transcript.style.opacity = 1;
                
                // When we get a final text transcript, play the buffered audio
                if (state.audioBufferQueue.length > 0) {
                    playBufferedAudio();
                }
            }
        }
        
        function playBufferedAudio() {
            if (state.audioBufferQueue.length === 0) return;
            
            const pcmData = new Blob(state.audioBufferQueue);

            // Create a WAV file from the raw PCM data and play it
            const wavBlob = createWavBlob(pcmData, 16000);
            const audioUrl = URL.createObjectURL(wavBlob);
            const audio = new Audio(audioUrl);
            audio.play();

            // Clear the buffer for the next utterance
            state.audioBufferQueue = [];
        }


        // --- Utility Functions ---

        function toggleMute() {
            if (!state.localStream) return;
            state.isMuted = !state.isMuted;
            state.localStream.getAudioTracks()[0].enabled = !state.isMuted;
            ui.muteBtn.querySelector('i').className = state.isMuted ? 'fa-solid fa-microphone-slash' : 'fa-solid fa-microphone';
        }

        function createWavBlob(pcmData, sampleRate) {
            const header = new ArrayBuffer(44);
            const view = new DataView(header);
            
            // RIFF identifier
            writeString(view, 0, 'RIFF');
            // file length
            view.setUint32(4, 36 + pcmData.size, true);
            // RIFF type
            writeString(view, 8, 'WAVE');
            // format chunk identifier
            writeString(view, 12, 'fmt ');
            // format chunk length
            view.setUint32(16, 16, true);
            // sample format (1 for PCM)
            view.setUint16(20, 1, true);
            // channel count
            view.setUint16(22, 1, true);
            // sample rate
            view.setUint32(24, sampleRate, true);
            // byte rate (sample rate * block align)
            view.setUint32(28, sampleRate * 2, true);
            // block align (channel count * bytes per sample)
            view.setUint16(32, 2, true);
            // bits per sample
            view.setUint16(34, 16, true);
            // data chunk identifier
            writeString(view, 36, 'data');
            // data chunk length
            view.setUint32(40, pcmData.size, true);

            return new Blob([header, pcmData], { type: 'audio/wav' });
        }
        
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }
    </script>
</body>
</html>